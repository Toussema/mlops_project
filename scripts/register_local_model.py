# scripts\register_local_model.py:
"""
Script corrig√© pour enregistrer le mod√®le Transformer dans MLflow
avec persistance garantie des artifacts
"""

import os

import mlflow
import torch
import transformers
from mlflow.tracking import MlflowClient
from transformers import AutoModelForSequenceClassification, AutoTokenizer

# --- Configuration ---
MLFLOW_TRACKING_URI = "file:///C:/Users/touha/Desktop/mlops_project/mlruns"
EXPERIMENT_NAME = "mlops_project_experiment"
MODEL_NAME = "TransformerTicketClassifier"
MODEL_PATH = "C:/Users/touha/Desktop/mlops_project/models/transformer"
TASK = "text-classification"


def register_existing_model():
    """Enregistrer le mod√®le Transformer avec persistance garantie des artifacts"""

    # Configuration MLflow
    mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)
    client = MlflowClient()

    # R√©cup√©rer l'exp√©rience
    try:
        experiment = mlflow.set_experiment(EXPERIMENT_NAME)
        print(f"‚úÖ Exp√©rience active: {experiment.name} ({experiment.experiment_id})")
    except Exception as e:
        print(f"‚ùå Erreur configuration exp√©rience: {e}")
        return

    # V√©rifier le mod√®le local
    if not os.path.exists(MODEL_PATH):
        print(f"‚ùå Mod√®le introuvable: {MODEL_PATH}")
        return

    print(f"üìÅ Mod√®le trouv√©: {MODEL_PATH}")

    # Cr√©er le run avec le mod√®le
    with mlflow.start_run(run_name="transformer_import_fixed") as run:
        run_id = run.info.run_id
        print(f"üöÄ Run cr√©√©: {run_id}")

        try:
            # Charger mod√®le et tokenizer
            print("üì• Chargement du mod√®le...")
            model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH)
            tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)

            # Logger les param√®tres
            mlflow.log_param("source", "local_import")
            mlflow.log_param("model_path", MODEL_PATH)
            mlflow.log_param("task", TASK)

            # Logger des m√©triques (valeurs indicatives)
            mlflow.log_metric("eval_accuracy", 0.90)
            mlflow.log_metric("eval_f1", 0.89)
            mlflow.log_metric("eval_precision", 0.88)
            mlflow.log_metric("eval_recall", 0.91)

            # Exemple d'input pour la signature
            sample_input = ["This is a test ticket"]

            # Logger le mod√®le avec signature explicite
            print("üì§ Logging du mod√®le dans MLflow...")
            model_info = mlflow.transformers.log_model(
                transformers_model={"model": model, "tokenizer": tokenizer},
                artifact_path="model",
                task=TASK,
                input_example=sample_input,
                pip_requirements=[
                    f"transformers=={transformers.__version__}",
                    f"torch=={torch.__version__}",
                    "torchvision",
                ],
            )

            print("‚úÖ Mod√®le logu√© avec succ√®s")
            print(f"   Model URI: {model_info.model_uri}")

        except Exception as e:
            print(f"‚ùå Erreur lors du logging: {e}")
            import traceback

            traceback.print_exc()
            return

        artifact_path = (
            f"C:/Users/touha/Desktop/mlops_project/mlruns/"
            f"{experiment.experiment_id}/{run_id}/artifacts/model"
        )
        if os.path.exists(artifact_path):
            print(f"‚úÖ Artifacts sauvegard√©s dans: {artifact_path}")
            files = os.listdir(artifact_path)
            print(f"   Fichiers: {files[:5]}...")  # Afficher les 5 premiers
        else:
            print(f"‚ö†Ô∏è ATTENTION: Artifacts non trouv√©s dans {artifact_path}")
            return

    print("\nüì¶ Enregistrement dans le Model Registry...")
    model_uri = f"runs:/{run_id}/model"

    try:
        model_version = mlflow.register_model(model_uri=model_uri, name=MODEL_NAME)
        print(f"‚úÖ Mod√®le enregistr√©: {MODEL_NAME} (version {model_version.version})")

        client.set_model_version_tag(
            name=MODEL_NAME,
            version=model_version.version,
            key="source",
            value="local_import",
        )

        # Promouvoir en Production ou alias 'champion'
        try:
            client.set_registered_model_alias(
                name=MODEL_NAME,
                alias="champion",
                version=model_version.version,
            )
            print(f"‚úÖ Alias 'champion' attribu√© √† la version {model_version.version}")
        except AttributeError:
            client.transition_model_version_stage(
                name=MODEL_NAME,
                version=model_version.version,
                stage="Production",
                archive_existing_versions=True,
            )
            print("‚úÖ Mod√®le promu en Production")

        print("\nüéâ Enregistrement termin√© avec succ√®s!")

    except Exception as e:
        print(f"‚ùå Erreur lors de l'enregistrement: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    register_existing_model()
