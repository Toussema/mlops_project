import os
os.environ["TRANSFORMERS_NO_TF"] = "1"
os.environ["TRANSFORMERS_NO_FLAX"] = "1"
os.environ["TRANSFORMERS_NO_JAX"] = "1"

from fastapi import FastAPI
from pydantic import BaseModel
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
from prometheus_client import Counter, Histogram, CollectorRegistry, generate_latest, CONTENT_TYPE_LATEST
import time
from fastapi.responses import Response

# ---------------------------------------------------
# ðŸ”¹ CrÃ©ation d'un registre Prometheus dÃ©diÃ©
# ---------------------------------------------------
registry = CollectorRegistry()

REQUEST_COUNT = Counter(
    'request_count',
    'Total number of prediction requests',
    registry=registry
)
REQUEST_LATENCY = Histogram(
    'request_latency_seconds',
    'Prediction request latency',
    registry=registry
)

# ---------------------------------------------------
# ðŸ”¹ Initialisation de lâ€™application FastAPI
# ---------------------------------------------------
app = FastAPI(title="Transformer Ticket Classifier API")

# ---------------------------------------------------
# ðŸ”¹ Chargement du modÃ¨le local Hugging Face
# ---------------------------------------------------
MODEL_PATH = "/content/drive/MyDrive/MLops/mlops_project/models/transformer"

# Mappage par dÃ©faut (au cas oÃ¹ le modÃ¨le nâ€™en contient pas)
LABEL_MAP = {
    0: "Access",
    1: "Administrative rights",
    2: "HR Support",
    3: "Hardware",
    4: "Internal Project",
    5: "Miscellaneous",
    6: "Purchase",
    7: "Storage"
}

print("ðŸš€ Chargement du modÃ¨le local...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH)
print("âœ… ModÃ¨le chargÃ© avec succÃ¨s !")

# Si le modÃ¨le a un mapping interne, on lâ€™utilise
if hasattr(model.config, "id2label") and model.config.id2label:
    LABEL_MAP = model.config.id2label
    print("ðŸ§­ Mapping du modÃ¨le utilisÃ© :", LABEL_MAP)

# ---------------------------------------------------
# ðŸ”¹ SchÃ©ma dâ€™entrÃ©e
# ---------------------------------------------------
class TicketInput(BaseModel):
    text: str

# ---------------------------------------------------
# ðŸ”¹ Endpoint de prÃ©diction
# ---------------------------------------------------
@app.post("/predict")
async def predict(ticket: TicketInput):
    REQUEST_COUNT.inc()
    start_time = time.time()

    inputs = tokenizer(ticket.text, return_tensors="pt", truncation=True, padding=True, max_length=128)
    with torch.no_grad():
        outputs = model(**inputs)
    logits = outputs.logits
    pred = torch.argmax(logits, dim=1).item()
    label = LABEL_MAP.get(pred, "Unknown")

    REQUEST_LATENCY.observe(time.time() - start_time)
    return {"prediction": label}

# ---------------------------------------------------
# ðŸ”¹ Endpoint Prometheus
# ---------------------------------------------------
@app.get("/metrics")
async def metrics():
    return Response(generate_latest(registry), media_type=CONTENT_TYPE_LATEST)
